# 调优案列分析与实战

## 高性能硬件上的程序部署策略

#### 情况概述:

在一个15万PV(Page View页面浏览量)/天左右的在线文档类型网站, 最近更换了硬件系统, 新的硬件为4个CPU,16GB物理内存, 操作系统为64位CentOS5.4, Resin作为Web服务器, 整个服务器暂时没有部署别的应用, 所有的硬件资源都可以提供给这访问量并不算太大的网站使用. 管理员为了尽量利用硬件选用了64位的JDK1.5, 并通过-Xmx和-Xms参数将Java堆的大小固定为12GB. 使用一段时间后发现使用效果并不理想, 网站经常不定期的出现长时间失去响应的情况.

#### 初步分析:

监控服务器运行状况后发现网站失去响应是由GC停顿导致的, 虚拟机运行在Server模式, 默认使用的是吞吐量优先收集器, 回收12GB的堆, 一次Full GC的停顿时间高达14秒, 并且由于程序设计的关系, 访问文档时需要把文档从磁盘提取到内存中, 导致内存中出现很多文档序列化产生的大对象, 这些大对象很多都进入了老年代, 没有在Minor GC中清理掉. 在这种情况下即使拥有12GB的堆, 内存也很快被消耗殆尽, 由此导致每隔十几分钟出现十几秒的停顿.

#### 拓展思考

先不考虑程序代码的问题, 程序部署上的主要问题显然是过大的堆内存进行回收时带来的长时间停顿. 硬件升级前使用32位系统1.5GB的堆, 用户只感觉使用网站比较缓慢, 但不会发生十分明显的卡顿, 因此才考虑升级硬件以提升性能, 如果重新缩小给Java堆分配的内存, 那么硬件上的投资就显得很浪费.
在高性能硬件上部署程序, 目前有两种方式:
+ 通过64位JDK来使用大内存
+ 使用若干个32位虚拟机建立逻辑集群来利用硬件资源

这里, 我们的管理员使用的是第一种方式. 对于用户交互性强, 对停顿时间敏感的系统, 采用第一种方式时, 有一个前提就是: 有把握将应用程序的Full GC频率控制的足够的低, 如十几个小时一次乃至一天一次, 这样我们就可以通过深夜定时执行Full GC操作甚至重启应用服务来保持内存可用空间维持在一个稳定的水平上.
大多数的网站形式的应用中, 主要对象的生存周期都应该是请求级或者页面级, 会话级和全局级的长生命对象相对较少. 只要代码合理, 应当都能实现在超大堆中正常使用而没有Full GC, 这样的话, 使用超大内存堆, 网站的响应速度才会比较有保证. 此外使用64位JDK来管理大内存, 还需要考虑以下问题:
+ 内存回收导致的长时间停顿.
+ 现阶段, 64位JDK性能测试结果普遍低于32位JDK.
+ 需要保证程序足够稳定, 因为这种应用要是产生堆溢出, 几乎无法产生转储快照, 哪怕产生了快照, 也几乎没办法分析.
+ 相同程序在64位JDK消耗的内存一般比32位JDK大, 由于指针膨胀, 数据类型对齐补白等因素导致的.

现阶段不少人采用第二种方式: 使用若干个32位虚拟机建立逻辑集群来利用硬件资源. 具体做法是: 在一台物理集群上启动多个应用服务器进程, 每个服务器进程分配不同的端口, 然后在前端搭建一个负载均衡器, 以反向代理的方式来分配访问请求.
考虑到一台物理机器上建立逻辑集群的目的仅仅是为了尽可能利用硬件资源, 并不需要关心状态保留, 热转移之类的高可用性需求, 也不需要保证每个虚拟机进程有绝对的负载均衡, 因此使用无Session复制的亲合式集群是一个相当不错的选择. 我们仅仅需要保证故障集群具备亲合性, 也就是均衡器按照一定的规则算法(一般根据SessionID分配)将一个固定用户的请求永远分配到固定的一个集群节点进行处理即可, 这样程序开发阶段也不用为集群环境做什么特别的考虑.
但是集群模式也有自己的缺点:
+ 尽量避免节点竞争全局资源, 最典型的就是磁盘竞争, 各个节点如果同时访问某个磁盘文件的话,很容易导致IO异常.
+ 很难最高效率地利用某些资源池, 如连接池, 一般每个节点都需要建立自己独立的连接池, 这样有可能导致一些节点池满了而另外一些节点仍有空余. 尽管可以使用集中式的JNDI, 但是也会带来一定的复杂性和额外的性能开销.
+ 每个节点仍然受到32位内存的限制, 在32位Windows平台只能使用2GB的内存, 考虑到堆以外的内存开销, 一般最大为1.5GB, 在Linux系统中, 一般可以提升到3GB接近4GB,但是仍然受到最高4GB(2^32)内存的限制.

#### 解决方式

建立5个32位JDK的逻辑集群, 每个进程按2GB内存计算(堆固定为1.5GB), 占用了10GB内存. 另外建立一个Apache服务作为前端均衡代理访问门户. 考虑到用户对响应速度比较关心, 并且文档服务的主要压力集中在磁盘和内存的访问, CPU资源敏感度较低, 因此采用CMS收集器进行垃圾回收.

## 集群间同步导致的内存溢出

#### 情况概述

一个基于B/S的MIS系统, 硬件为两台2个CPU,8GB内存的HP小型机, 服务器是WebLogic 9.2, 每台机器启动了3个WebLogic实例, 构成一个6个节点的亲合式集群. 由于是亲合式集群, 节点之间没有进行Session同步, 但是有一些需求要实现部分数据在各个节点间共享. 开始这些数据存放在数据库中, 但是由于读写频繁竞争很激烈, 性能影响较大, 后面使用JBossCache构建了一个全局缓存. 全局缓存启用之后, 服务正常使用了一段时间之后, 但最近却不定期的出现多次内存溢出的问题.

#### 初步分析

内存溢出异常不出现的时候, 服务器内存回收状况一直正常, 每次内存回收后都能恢复到一个稳定的可用空间, 开始怀疑是程序某些不常用的代码路径中存在内存泄漏, 但管理员反映最近程序并未更新或者升级过, 也没有什么特别操作. 只好让服务带着-XX:+HeapDumpOnOutOfMemeoryError参数允许一段时间. 最近一次溢出之后, 管理员发回了heapdump文件, 发现里面存在着大量org.jgroups.protocols.pbcast.NAKACK对象.

#### 拓展思考

JBossCache是基于自家的JGroups进行集群间的数据通信, JGroups使用协议栈的方式来实现收发数据包的各种所需特性自由组合, 数据包接受和发送时要经过每层协议栈的up()和down()方法, 其中的NAKACK栈用于保障各个包的有效顺序及重发. 由于消息有传输失败需要重发的可能性, 在确认所有注册在GMS(Group Member Service)的节点都收到正确的信息前, 发送的消息必须在内存中保留. 而此MIS的服务端中有一个负责安全校验的全局Filter, 每当接收到请求时, 均会更新一次最后操作时间, 且将这个时间同步到所有的节点去, 使得一个用户在一段时间内不能在多台机器上登录, 服务使用过程中, 往往一个页面会产生数次乃至数十次的请求, 因此这个过滤器导致集群各个节点之间网络交互非常频繁. 当网络情况不能满足传输要求时, 重发数据在内存中不断累积, 很快就导致内存溢出.

#### 解决方式

这一类被集群共享的数据要使用类似JBossCache这种集群缓存来同步的话, 可以允许读操作频繁, 因为数据在本地内存有一份副本, 读取动作不会耗费多少资源, 但不应该有过于频繁的写操作, 那样会带来很大的网络同步的开销.

## 堆外内存导致的溢出错误

#### 情况概述

一个学校的小型项目: 基于B/S的电子考试系统, 为了实现客户端能实时地从服务端接收考试数据, 系统使用了逆向的Ajax技术,选用了CometD1.1.1作为服务端推送框架, 服务器是Jetty7.1.4, 硬件为一台普通的PC机i5 CPU, 4GB内存, 运行32位Windows操作系统. 测试区间发现服务端不定时抛出内存溢出异常, 服务器不一定每次都会出现异常. 但是如果正式考试时崩溃一次, 估计整场考试都会乱套.

#### 初步分析

网站管理员尝试将堆开到最大1.6GB, 但是发现开到最大基本没有效果, 好像还更加频繁了. 添加-XX:+HeapDumpOnOutOfMemoryError, 在内存溢出的时候, 居然没有什么文件产生. 无奈只好一直挂着jstat并一直紧盯着屏幕, GC并不频繁, Eden去,Survivor区,老年代等都很正常, 但就是不停地抛出内存溢出异常. 最后在内存溢出系统日志中找到异常堆栈:

```java
[org.eclipse.jetty.util.log] handle failed java.lang.OutOfMemoryError: null
	at sun.misc.Unsafe.allocateMemory(Native Method)
    at java.nio.DirectByteBuffer.<init>(DirectByteBuffer.java:99)
    at java.nio.ByteBuffer.allocateDirect(ByteBuffer.java:288)
    at org.eclipse.jetty.io.nio.DirectNIOBuffer.<init>
    ...
```

#### 拓展思考

看到这里我们基本明白哪里出现问题了: 直接内存(Direct Memeory). 这台服务器使用的32位Windows平台的限制是2GB, 其中划了1.6GB给Java堆, 而Direct Memory内存并不算入1.6GB的堆之内, 因此他最大也只能在剩余的0.4GB空间中分出一部分. 这里的出现内存溢出的情况是:  垃圾收集进行时, 虚拟机虽然会对Direct Memory进行回收, 但是Direct Memory却不能像新生代, 老年代那样, 发现空间不足了就通知收集器进行垃圾回收, 它只能等待老年代满了后Full GC, 然后`顺便地`帮它清理掉内存的废弃对象. 不然它只能一直等到抛出内存溢出异常时, 先catch掉, 再在catch块中里面大喊一声`System.gc()`.要是虚拟机还是不听(比如虚拟机打开了-XX:+DisableExplicitGC), 那就只能眼睁睁地看着堆中还有许多空余内存, 自己不得不抛出内存溢出异常. 这个实例中使用CometD1.1.1框架, 正好有大量的NIO操作需要使用到DirectMemeory内存.
除了Java堆和永久代之外, 还有一些区域会占用较多的内存, 所以这里区域的内存总和收到操作系统进程最大内存的限制:
+ Direct Memory: 可以通过-XX:MaxDirectMemorySize调整大小, 内存不足时抛出OutOfMemoryError或者OutOfMemoryError:Direct buffer memory.
+ 线程堆栈: 可通过-Xss调整大小, 内存不足时抛出StackOverflowError(纵向无法分配, 即无法分配新的栈帧)或者OutOfMemoryError: unable to create new native thread(横向无法分配, 即无法建立新的线程).
+ Socket缓存区: 每个Socket连接都Receive和Send两个缓存区, 分别占据37KB和25KB内存, 连接多的话这块内存占用也较为客观. 如果无法分配, 这可能会抛出IOException: Too many open files异常.
+ JNI代码: 如果代码使用JNI调用本地库, 那本地库使用的内存也不再在堆中.
+ 虚拟机和GC: 虚拟机, GC的代码执行也需要消耗一定的内存.

## 外部命令导致系统缓慢

#### 情况概述

一个数字校园应用系统, 运行在一台4个CPU的Solaris10的操作系统上, 中间件为GlassFish服务器. 系统在做大并发压力测试的时候, 发现请求响应时间较慢, 通过操作系统的mpstat工具发现CPU使用率很高, 并且系统占用绝大多数的CPU资源的程序不是应用系统本身. 这是一个不正常的现象, 通常情况下用户应用的CPU占有率应该占主要地位, 才能说明系统是正常工作的.

#### 初步分析

通过Solaris10的Dtrace脚本可以查看当前情况下那些系统调用花费了最多的CPU资源, Dtrace运行后发现最消耗CPU资源的居然是`fork`系统调用, 众所周知, `fork`调用时Linux用来创建新进程的, 在Java虚拟机中, 用户编写的Java代码最多只有线程的概念, 不应当有进程的产生.
通过本系统的开发人员, 最终找到了答案: 每个用户请求的处理都需要执行一个外部的Shell脚本来获得系统的一些信息. 执行这个Shell脚本呢是通过Runtime.getRuntime().exec()方法来调用的. 这种调用方式可以达到目的, 但是它在虚拟机中是非常消耗资源的操作, 即使外部命令本身很快就可以执行完毕, 频繁调用时创建进程开销也非常可观. Java虚拟机执行这个命令的过程是: 首先克隆一个和当前虚拟机拥有一样的环境变量的进程, 然后再用这个新的进程去执行这个外部命令, 最后退出这个进程. 如果频繁执行这个操作, 系统的消耗就会很大, 不仅是CPU, 内存负担也很重.

#### 解决方案

用户更加建议去掉这个Shell脚本, 改为使用Java的API去获取这些信息, 系统恢复正常.

## 服务器JVM进程崩溃

#### 情况概述

一个基于B/S的MIS系统, 硬件为2台2个CPU,8GB内存的HP系统, 服务器为WebLogic9.2(第二个实例, 那套系统). 正常运行一段时间后, 最近发现在运行区间频繁出现集群节点的虚拟机进程自动关闭的现象, 留下一个hs_err_pid###.log文件后, 进程就消失了, 两台物理机里的每个节点都出现过进程崩溃的现象. 从系统日志中, 可以看出, ,每个节点崩溃前不久, 都出现过大量相同的异常:

```java
java.net.SocketException: Connection reset
at java.net.SocketInputStream.read(SocketInputStream.java:168)
at java.io.BufferedInputStream.fill(BufferedInputStream.java:218)
at java.io.BufferedInputStream.read(BufferedInputStream.java:235)
at org.apache.axis.transport.http.HTTPSender.readHeadersFromSocket(HTTPSender.java:583)
at org.apache.axis.transport.http.HTTPSender.invoke(HTTPSender.java:143)
...
```

#### 初步分析

这是远端断开连接的异常, 通过系统管理员了解到系统最近与一个OA门户做了集成, 在MIS系统工作流的代办事项变化时, 需要通过Web服务通知OA门户系统, 把代办事项的变化同步到OA门户之中. 通过SoapUI测试了一下同步待办事项的几个Web服务, 发现调用居然需要长达3分钟才能返回, 而且返回的结果都是连接中断.
由于MIS用户众多, 待办事项变化也很快, 为了不被OA系统速度所拖慢, 使用了异步的方式调用Web服务, 但由于两把服务速度不对等, 时间越长就累积越多Web服务没有调用完成, 导致在等待的线程和Socket连接越来越多, 最终超过了虚拟机的承受能力后使得虚拟机进程崩溃.

#### 解决方法

通知OA门户方修复无法使用的集成接口, 并将异步调用改为生产者/消费者模式的消息队列实现, 系统恢复正常.

## 不恰当数据结构导致内存占用过大

#### 情况概述

有一个后台RPC服务器, 使用64位虚拟机, 内存配置为-Xms4g(初始化内存为4g) -Xmx8g(最大分配内存8g) -Xmn1g(年轻代1g), 使用ParNew + CMS的收集器组合. 平时对外服务的Minor GC时间约在30毫秒以内, 完全可以接受. 但是业务上需要每10分钟加载一个约80MB的数据文件到内存中进行数据分析, 这些数据会在内存总形成超过100万个HashMap<Long, Long>Entry, 在这段时间里, Minor GC就会造成超过500毫秒的停顿, 对这个停顿时间就接受不了了. 具体情况:

```java
(Heap before GC invocations = 95 (full 4) :
par new generation total 903168K [0x00002aaaae770000, 0x00002aaaebb70000, 0x00002aaaebb70000)
eden space 802816K, 100% used [0x00002aaaae77000, 0x00002aaadf770000, 0x00002aaadf770000)
from space 100352K, 0% used [0x00002aaae5970000, 0x00002aaae59c1910, 0x00002aaaebb70000)
to space 100352K, 0% use [....]
concurrent mark-sweep generation, total 5845540K, used 3898978K[...]
concurrent-mark-sweep perm gen total 65536K, used 40333K[...]
2011-10-28T11:40:45.162+0800:226.504: [GC 226.504: [ParNew: 803142K -> 100352K(903168K), 0.5995670secs] 4702120K->4056332K(6748708K), 0.5997560secs][Times: user=1.46 sys=0.04, real=0.60secs]
Heap after GC invocations = 96 (full 4):
par new generation total 903168K, used 100352K [...]
eden space 802816K, 0% used [...]
from space 100352, 100% used[...]
to space 100352, 0% used[..,]
concurrent mark-sweep generation total 5845540K, used 3955980K[...]
concurrent-mark-sweep perm gen total 65536K, used 4033K[...]
Total time for which application threads were stopped: 0.6070570 seconds.
```

#### 初步分析

平时的Minor GC时间很短, 原因是新生代的对象都是可以清除的, 在MinorGC之后, Eden和Survivor基本上就处于完全空闲的状态, 而在分析数据文件的期间, 800MB的Eden空间很快被填满从而引发GC, 但MinorGC之后, 新生代大部分的对象又是存活的. 我们知道ParNew收集器采用的是复制算法, 这个算法的基础是建立在对象大多是`朝生夕灭`的特性上, 如果存活数量过多, 就把这些对象复制到Survivor并维持这些对象的引用的正确就成为了一个沉重的负担, 因此导致GC暂停时间明显变长.

#### 解决方式

如果不修改程序, 仅仅从GC调优的角度去解决这个问题, 可以考虑将Survivor空间去掉,(加入参数-XX:SurvivorRatio=65536(Eden和Survivor区的比例), -XX:MaxTenuringThreshold=0(经过MinorGC次数,就进入老年区)或者-XX:+AlwaysTenure(等价)). 这个方法治标不治本, 治本的方法还是修改程序, 改变数据的存储方式. 80MB的数据, 却占据了800MB的Eden区, 说明数据的储存效率很低:
HashMap<Long,Long>结构中, 只有key和value中存放的两个长整数是有效数据,(暂时不考虑TreeNode的存储情况,浪费更大), 共16B(2 x 8B). 这两个长整型数据包装成Long对象, 分别添加了8B的Mark Word和8B的Class指针, 在加8B存储数据的long值. 两个Long对象组成的Map.Entry之后, 又添加了16B的对象头, 然后一个8B的next字段和4B的int型hash字段, 为了对齐, 还必须添加4B的空白补充, 最后还有HashMap中table[]中存放着这个Entry的8B的引用. 实际消耗的内存: Long(24B) x 2 + Entry(32B) + Table Ref(8B) = 88B, 空间效率为: 16 / 88 = 18%. 是非常低的.

## 由Windows虚拟内存导致的长时间停顿

#### 情况概述

有一个带心跳检测功能的GUI桌面程序, 每15秒会发送一次心跳检测信号, 如果对方30秒以内都没有信号返回, 那就认为和对方程序连接已经断开. 程序上线之后发现, 心跳检测有误报的情况, 查询日志发现误报的原因是程序会偶尔出现间隔约一分钟左右的完全无日志输出, 处于停顿状态.

#### 初步分析

经过添加参数-XX:+PringGCApplicationStoppedTime -XX:+PrintGCDateStamps -Xloggc: gclog.log后, 从GC日志文件中发现停顿确实是由GC导致, 大部分GC时间都控制在100毫秒以为, 但偶尔会出现1分钟的GC.
从GC日志中找到了长时间停顿的具体日志信息(添加了-XX:+PrintReferenceGC参数) 找到的日志片段如下所示. 从日志中可以看出, 真正执行GC动作的时间并不是很长, 但是准备开始GC到真正开始GC之间消耗的时间占据绝大部分. 观察到GUI程序内存发生变化的一个特点就是, 当它最小化的时候, 资源管理器显示占用内存大幅度减少, 但是虚拟内存并没有变化, 因此怀疑程序在最小化时, 它的工作内存被自动交换到磁盘的页面文件之中, 这样发生GC时就有可能因为恢复页面文件操作导致不正常的GC停顿.

#### 解决方式

运行时添加`-Dsun.awt.keepWorkingSetOnMinimize=true`,可以得到解决.
